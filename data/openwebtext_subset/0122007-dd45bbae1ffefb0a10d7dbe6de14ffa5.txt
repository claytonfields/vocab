

The GNU Wget is a free utility for non-interactive download of files from the Web. It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. Recently, I was downloading a Ubuntu Linux ISO (618 MB) file for testing purpose at my home PC. My Uninterrupted Power Supply (UPS) unit was not working. I started download with the following wget command:



$ wget http://ftp.ussg.iu.edu/linux/ubuntu-releases/5.10/ubuntu-5.10-install-i386.iso

However, due to power supply problem, my computer rebooted at 98% download. Again, after reboot I typed wget at a shell prompt:

$ wget http://ftp.ussg.iu.edu/linux/ubuntu-releases/5.10/ubuntu-5.10-install-i386.iso

However, wget restarted to download ISO image from scratch again. I thought wget should resume partially downloaded ISO file.

Adblock detected ðŸ˜± PayPal/Bitcoin, or become a supporter using Patreon. My website is made possible by displaying online advertisements to my visitors. I get it! Ads are annoying but they help keep this website running. It is hard to keep the site running and producing new content when so many people block ads. Please consider donating money to the nixCraft via, or become a

wget resume download

After reading wget(1), I found the -c or --continue option to continue getting a partially downloaded file. This is useful when you want to finish a download started by a previous instance of wget, or by another program. The syntax is:

wget -c url wget --continue url wget --continue [ options ] url wget -c url wget --continue url wget --continue [options] url

So I decided to continue getting a partially-downloaded ubuntu-5.10-install-i386.iso file using the following command:

$ wget -c http://ftp.ussg.iu.edu/linux/ubuntu-releases/5.10/ubuntu-5.10-install-i386.iso

OR

$ wget --continue http://ftp.ussg.iu.edu/linux/ubuntu-releases/5.10/ubuntu-5.10-install-i386.iso

Sample session:



Make sure your run wget command in the same directory where the first download started. If there is a file named ubuntu-5.10-install-i386.iso in the current directory, Wget will assume that it is the first portion of the remote file, and will ask the server to continue the retrieval from an offset equal to the length of the local file. Thus, it will result in saving both time and bandwidth.

For more information about the wget, read man pages:

$ man wget

$ wget --help

See also:

Share on Facebook Twitter